{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models for Customer Upsell Prediction\n",
    "## AI Customer Upsell Prediction System\n",
    "\n",
    "This notebook implements:\n",
    "- Custom PyTorch Neural Networks\n",
    "- Autoencoder for feature learning\n",
    "- Deep TabNet architecture\n",
    "- Attention mechanisms\n",
    "- Advanced regularization techniques\n",
    "- GPU-accelerated training\n",
    "- Model interpretation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Advanced architectures\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    TABNET_AVAILABLE = True\n",
    "    print(\"‚úÖ TabNet available\")\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TabNet not available, will use custom architectures\")\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Data for Deep Learning\n",
    "print(\"üìä Loading Data for Deep Learning Models...\")\n",
    "\n",
    "# Load processed data\n",
    "df = pd.read_csv('../data/processed/telecom_processed.csv')\n",
    "feature_columns = joblib.load('../models/feature_columns.pkl')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[feature_columns].fillna(0)\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "y = df['Churn_Binary']\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Churn Rate: {y.mean():.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Splits:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Scale features for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features scaled for deep learning\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "print(f\"‚úÖ Data converted to PyTorch tensors on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Deep Neural Network Architecture\n",
    "print(\"üß† Defining Custom Deep Neural Network Architectures\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class DeepCustomerNet(nn.Module):\n",
    "    \"\"\"Custom Deep Neural Network for Customer Upsell Prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64, 32], dropout_rate=0.3):\n",
    "        super(DeepCustomerNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            # Dropout\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "\n",
    "class AttentionCustomerNet(nn.Module):\n",
    "    \"\"\"Neural Network with Attention Mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_heads=8):\n",
    "        super(AttentionCustomerNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embedding = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embed features\n",
    "        embedded = self.feature_embedding(x)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        embedded = embedded.unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "        \n",
    "        # Apply attention\n",
    "        attended, attention_weights = self.attention(embedded, embedded, embedded)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        attended = attended.squeeze(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(attended)\n",
    "        \n",
    "        return output.squeeze(), attention_weights\n",
    "\n",
    "\n",
    "class ResidualCustomerNet(nn.Module):\n",
    "    \"\"\"Neural Network with Residual Connections\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_blocks=3):\n",
    "        super(ResidualCustomerNet, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            self._make_residual_block(hidden_size) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def _make_residual_block(self, hidden_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.residual_blocks:\n",
    "            residual = x\n",
    "            x = block(x)\n",
    "            x = F.relu(x + residual)  # Residual connection\n",
    "        \n",
    "        # Output\n",
    "        return self.output_layers(x).squeeze()\n",
    "\n",
    "print(\"‚úÖ Custom neural network architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder for Feature Learning\n",
    "print(\"üîÑ Implementing Autoencoder for Feature Learning\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "class CustomerAutoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder for learning compressed customer representations\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, encoding_sizes=[32, 16, 8]):\n",
    "        super(CustomerAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.encoding_sizes = encoding_sizes\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for encoding_size in encoding_sizes:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_size, encoding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_size = encoding_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers[:-1])  # Remove last dropout\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        decoding_sizes = encoding_sizes[::-1][1:] + [input_size]  # Reverse and add input size\n",
    "        \n",
    "        prev_size = encoding_sizes[-1]\n",
    "        for i, decoding_size in enumerate(decoding_sizes):\n",
    "            decoder_layers.append(nn.Linear(prev_size, decoding_size))\n",
    "            if i < len(decoding_sizes) - 1:  # No activation on final layer\n",
    "                decoder_layers.extend([nn.ReLU(), nn.Dropout(0.2)])\n",
    "            prev_size = decoding_size\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        return self.decoder(encoded)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for probabilistic feature learning\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, latent_size=16, hidden_size=64):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(hidden_size // 2, latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_size // 2, latent_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar, z\n",
    "\n",
    "print(\"‚úÖ Autoencoder architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities and Loss Functions\n",
    "print(\"‚öôÔ∏è Setting Up Training Utilities\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=100, early_stopping=None, device='cpu'):\n",
    "    \"\"\"Generic model training function\"\"\"\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Handle different model outputs\n",
    "            if isinstance(model, AttentionCustomerNet):\n",
    "                outputs, _ = model(batch_X)\n",
    "            else:\n",
    "                outputs = model(batch_X)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                if isinstance(model, AttentionCustomerNet):\n",
    "                    outputs, _ = model(batch_X)\n",
    "                else:\n",
    "                    outputs = model(batch_X)\n",
    "                \n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_auc = roc_auc_score(all_targets, all_outputs)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}, \"\n",
    "                  f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training utilities set up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder for Feature Learning\n",
    "print(\"üîÑ Training Autoencoder for Feature Learning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Data loaders created (batch size: {batch_size})\")\n",
    "\n",
    "# Initialize autoencoder\n",
    "input_size = X_train_tensor.shape[1]\n",
    "autoencoder = CustomerAutoencoder(input_size, encoding_sizes=[64, 32, 16]).to(device)\n",
    "\n",
    "# Training setup\n",
    "ae_criterion = nn.MSELoss()\n",
    "ae_optimizer = optim.Adam(autoencoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "ae_scheduler = ReduceLROnPlateau(ae_optimizer, mode='min', factor=0.5, patience=10)\n",
    "ae_early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "print(\"üöÄ Training autoencoder...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Custom training loop for autoencoder\n",
    "autoencoder.train()\n",
    "ae_train_losses = []\n",
    "ae_val_losses = []\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    for batch_X, _ in train_loader:  # Don't need labels for autoencoder\n",
    "        batch_X = batch_X.to(device)\n",
    "        \n",
    "        ae_optimizer.zero_grad()\n",
    "        reconstructed, encoded = autoencoder(batch_X)\n",
    "        loss = ae_criterion(reconstructed, batch_X)\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    autoencoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            reconstructed, encoded = autoencoder(batch_X)\n",
    "            loss = ae_criterion(reconstructed, batch_X)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    autoencoder.train()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    ae_train_losses.append(train_loss)\n",
    "    ae_val_losses.append(val_loss)\n",
    "    \n",
    "    ae_scheduler.step(val_loss)\n",
    "    ae_early_stopping(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if ae_early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚úÖ Autoencoder training completed in {training_time:.1f} seconds\")\n",
    "\n",
    "# Extract learned features\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_encoded = autoencoder.encode(X_train_tensor).cpu().numpy()\n",
    "    X_val_encoded = autoencoder.encode(X_val_tensor).cpu().numpy()\n",
    "    X_test_encoded = autoencoder.encode(X_test_tensor).cpu().numpy()\n",
    "\n",
    "print(f\"‚úÖ Features encoded: {X_train_tensor.shape[1]} ‚Üí {X_train_encoded.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Deep Neural Networks\n",
    "print(\"üß† Training Deep Neural Network Models\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DeepCustomerNet': DeepCustomerNet(\n",
    "        input_size=input_size,\n",
    "        hidden_sizes=[256, 128, 64, 32],\n",
    "        dropout_rate=0.3\n",
    "    ).to(device),\n",
    "    \n",
    "    'AttentionNet': AttentionCustomerNet(\n",
    "        input_size=input_size,\n",
    "        hidden_size=128,\n",
    "        num_heads=8\n",
    "    ).to(device),\n",
    "    \n",
    "    'ResidualNet': ResidualCustomerNet(\n",
    "        input_size=input_size,\n",
    "        hidden_size=128,\n",
    "        num_blocks=3\n",
    "    ).to(device)\n",
    "}\n",
    "\n",
    "# Add autoencoder-based model\n",
    "class AutoencoderClassifier(nn.Module):\n",
    "    def __init__(self, autoencoder, encoded_size):\n",
    "        super(AutoencoderClassifier, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(encoded_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Freeze autoencoder weights\n",
    "        for param in self.autoencoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            encoded = self.autoencoder.encode(x)\n",
    "        return self.classifier(encoded).squeeze()\n",
    "\n",
    "models['AutoencoderNet'] = AutoencoderClassifier(autoencoder, X_train_encoded.shape[1]).to(device)\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(models)} deep learning models\")\n",
    "\n",
    "# Training results storage\n",
    "training_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüöÄ Training {model_name}...\")\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)  # Handle class imbalance\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "    early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=100,\n",
    "        early_stopping=early_stopping,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} training completed in {training_time:.1f} seconds\")\n",
    "    print(f\"   Best Validation AUC: {results['best_val_auc']:.4f}\")\n",
    "    \n",
    "    training_results[model_name] = results\n",
    "    training_results[model_name]['training_time'] = training_time\n",
    "    trained_models[model_name] = model\n",
    "\n",
    "print(f\"\\nüéâ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet Implementation (if available)\n",
    "if TABNET_AVAILABLE:\n",
    "    print(\"üìä Training TabNet Model\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Prepare data for TabNet (requires numpy arrays)\n",
    "    X_train_np = X_train_scaled\n",
    "    X_val_np = X_val_scaled\n",
    "    X_test_np = X_test_scaled\n",
    "    y_train_np = y_train.values\n",
    "    y_val_np = y_val.values\n",
    "    y_test_np = y_test.values\n",
    "    \n",
    "    # Initialize TabNet\n",
    "    tabnet_model = TabNetClassifier(\n",
    "        n_d=32,  # Width of the decision prediction layer\n",
    "        n_a=32,  # Width of the attention embedding for each mask\n",
    "        n_steps=3,  # Number of successive steps in the architecture\n",
    "        gamma=1.3,  # Coefficient for feature reusage in the masks\n",
    "        cat_idxs=[],  # No categorical features\n",
    "        cat_dims=[],  # No categorical features\n",
    "        cat_emb_dim=1,\n",
    "        lambda_sparse=1e-3,  # Sparsity regularization\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "        mask_type='sparsemax',\n",
    "        scheduler_params=dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.9),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose=1,\n",
    "        device_name=str(device)\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Training TabNet...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train TabNet\n",
    "    tabnet_model.fit(\n",
    "        X_train=X_train_np,\n",
    "        y_train=y_train_np,\n",
    "        eval_set=[(X_val_np, y_val_np)],\n",
    "        eval_name=['validation'],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    tabnet_training_time = time.time() - start_time\n",
    "    \n",
    "    # Get TabNet predictions\n",
    "    tabnet_val_preds = tabnet_model.predict_proba(X_val_np)[:, 1]\n",
    "    tabnet_val_auc = roc_auc_score(y_val_np, tabnet_val_preds)\n",
    "    \n",
    "    print(f\"‚úÖ TabNet training completed in {tabnet_training_time:.1f} seconds\")\n",
    "    print(f\"   Validation AUC: {tabnet_val_auc:.4f}\")\n",
    "    \n",
    "    # Add to results\n",
    "    training_results['TabNet'] = {\n",
    "        'best_val_auc': tabnet_val_auc,\n",
    "        'training_time': tabnet_training_time\n",
    "    }\n",
    "    trained_models['TabNet'] = tabnet_model\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TabNet not available, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate All Models on Test Set\n",
    "print(\"üìä Evaluating All Models on Test Set\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    print(f\"\\nüîÑ Evaluating {model_name}...\")\n",
    "    \n",
    "    if model_name == 'TabNet' and TABNET_AVAILABLE:\n",
    "        # TabNet evaluation\n",
    "        y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        y_true = y_test_np\n",
    "        \n",
    "    else:\n",
    "        # PyTorch model evaluation\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                if isinstance(model, AttentionCustomerNet):\n",
    "                    outputs, _ = model(batch_X)\n",
    "                else:\n",
    "                    outputs = model(batch_X)\n",
    "                \n",
    "                probabilities = outputs.cpu().numpy()\n",
    "                predictions = (probabilities > 0.5).astype(int)\n",
    "                \n",
    "                all_probabilities.extend(probabilities)\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        y_pred_proba = np.array(all_probabilities)\n",
    "        y_pred = np.array(all_predictions)\n",
    "        y_true = np.array(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    test_results[model_name] = {\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Training_Time': training_results[model_name]['training_time']\n",
    "    }\n",
    "    \n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(test_results).T\n",
    "results_df = results_df.sort_values('AUC', ascending=False)\n",
    "\n",
    "print(f\"\\nüèÜ Deep Learning Model Performance Summary:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_dl_model = results_df.index[0]\n",
    "best_dl_auc = results_df.iloc[0]['AUC']\n",
    "\n",
    "print(f\"\\nü•á Best Deep Learning Model: {best_dl_model} (AUC: {best_dl_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability and Feature Analysis\n",
    "print(\"üîç Deep Learning Model Interpretability\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Attention Visualization (if AttentionNet performed well)\n",
    "if 'AttentionNet' in trained_models:\n",
    "    print(\"\\nüëÅÔ∏è Analyzing Attention Weights...\")\n",
    "    \n",
    "    attention_model = trained_models['AttentionNet']\n",
    "    attention_model.eval()\n",
    "    \n",
    "    # Get attention weights for a sample\n",
    "    sample_batch = X_test_tensor[:100]  # First 100 test samples\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, attention_weights = attention_model(sample_batch)\n",
    "    \n",
    "    # Average attention weights across samples and heads\n",
    "    avg_attention = attention_weights.mean(dim=(0, 1)).cpu().numpy()  # Average across batch and heads\n",
    "    \n",
    "    print(f\"‚úÖ Attention analysis completed\")\n",
    "    print(f\"   Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Feature Importance using Gradient-based methods\n",
    "def get_feature_importance_gradients(model, X_sample, feature_names):\n",
    "    \"\"\"Calculate feature importance using gradients\"\"\"\n",
    "    model.eval()\n",
    "    X_sample.requires_grad_()\n",
    "    \n",
    "    if isinstance(model, AttentionCustomerNet):\n",
    "        output, _ = model(X_sample)\n",
    "    else:\n",
    "        output = model(X_sample)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients = torch.autograd.grad(output.sum(), X_sample, create_graph=True)[0]\n",
    "    \n",
    "    # Feature importance as absolute gradient values\n",
    "    importance = torch.abs(gradients).mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# Calculate feature importance for best model\n",
    "if best_dl_model != 'TabNet':\n",
    "    print(f\"\\nüìä Calculating feature importance for {best_dl_model}...\")\n",
    "    \n",
    "    best_model = trained_models[best_dl_model]\n",
    "    sample_data = X_test_tensor[:1000]  # Use 1000 samples\n",
    "    \n",
    "    feature_importance = get_feature_importance_gradients(\n",
    "        best_model, sample_data, feature_columns\n",
    "    )\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ Top 15 Most Important Features ({best_dl_model}):\")\n",
    "    print(importance_df.head(15))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    bars = plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance (Gradient-based)')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_dl_model}', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, top_features['Importance'])):\n",
    "        plt.text(value + 0.001, i, f'{value:.4f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif TABNET_AVAILABLE and 'TabNet' in trained_models:\n",
    "    print(f\"\\nüìä TabNet Feature Importance Analysis...\")\n",
    "    \n",
    "    # TabNet has built-in feature importance\n",
    "    tabnet_importance = trained_models['TabNet'].feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': tabnet_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ Top 15 Most Important Features (TabNet):\")\n",
    "    print(importance_df.head(15))\n",
    "    \n",
    "    # Visualize TabNet feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    bars = plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "                    color=plt.cm.plasma(np.linspace(0, 1, len(top_features))))\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - TabNet', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History Visualization\n",
    "print(\"üìà Visualizing Training History\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create training history plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Deep Learning Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "model_names = [name for name in training_results.keys() if name != 'TabNet']\n",
    "\n",
    "# Training Loss\n",
    "for i, model_name in enumerate(model_names):\n",
    "    if 'train_losses' in training_results[model_name]:\n",
    "        axes[0,0].plot(training_results[model_name]['train_losses'], \n",
    "                      label=model_name, color=colors[i % len(colors)], alpha=0.7)\n",
    "\n",
    "axes[0,0].set_title('Training Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "for i, model_name in enumerate(model_names):\n",
    "    if 'val_losses' in training_results[model_name]:\n",
    "        axes[0,1].plot(training_results[model_name]['val_losses'], \n",
    "                      label=model_name, color=colors[i % len(colors)], alpha=0.7)\n",
    "\n",
    "axes[0,1].set_title('Validation Loss')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation AUC\n",
    "for i, model_name in enumerate(model_names):\n",
    "    if 'val_aucs' in training_results[model_name]:\n",
    "        axes[1,0].plot(training_results[model_name]['val_aucs'], \n",
    "                      label=model_name, color=colors[i % len(colors)], alpha=0.7)\n",
    "\n",
    "axes[1,0].set_title('Validation AUC')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('AUC Score')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model Performance Comparison\n",
    "model_names_all = list(test_results.keys())\n",
    "auc_scores = [test_results[name]['AUC'] for name in model_names_all]\n",
    "training_times = [test_results[name]['Training_Time'] for name in model_names_all]\n",
    "\n",
    "bars = axes[1,1].bar(range(len(model_names_all)), auc_scores, \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(model_names_all))))\n",
    "axes[1,1].set_title('Test AUC Comparison')\n",
    "axes[1,1].set_xticks(range(len(model_names_all)))\n",
    "axes[1,1].set_xticklabels(model_names_all, rotation=45, ha='right')\n",
    "axes[1,1].set_ylabel('AUC Score')\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = model_names_all.index(best_dl_model)\n",
    "bars[best_idx].set_color('gold')\n",
    "bars[best_idx].set_edgecolor('red')\n",
    "bars[best_idx].set_linewidth(2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, auc_scores):\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autoencoder reconstruction visualization\n",
    "print(\"\\nüîÑ Autoencoder Reconstruction Analysis...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot autoencoder training history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ae_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(ae_val_losses, label='Validation Loss', color='red')\n",
    "plt.title('Autoencoder Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot reconstruction quality\n",
    "plt.subplot(1, 2, 2)\n",
    "with torch.no_grad():\n",
    "    sample_input = X_test_tensor[:100]\n",
    "    reconstructed, _ = autoencoder(sample_input)\n",
    "    reconstruction_error = F.mse_loss(reconstructed, sample_input, reduction='none').mean(dim=1).cpu().numpy()\n",
    "\n",
    "plt.hist(reconstruction_error, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('MSE Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training history visualization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Impact Analysis with Deep Learning Models\n",
    "print(\"üíº Business Impact Analysis - Deep Learning Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use best deep learning model for business analysis\n",
    "if best_dl_model != 'TabNet':\n",
    "    best_model = trained_models[best_dl_model]\n",
    "    best_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if isinstance(best_model, AttentionCustomerNet):\n",
    "            final_predictions, _ = best_model(X_test_tensor)\n",
    "        else:\n",
    "            final_predictions = best_model(X_test_tensor)\n",
    "        \n",
    "        final_predictions = final_predictions.cpu().numpy()\n",
    "\n",
    "elif TABNET_AVAILABLE and best_dl_model == 'TabNet':\n",
    "    final_predictions = trained_models['TabNet'].predict_proba(X_test_np)[:, 1]\n",
    "\n",
    "# Business calculations\n",
    "avg_customer_value = df['Total_Charges'].mean()\n",
    "annual_customer_value = avg_customer_value * 12\n",
    "intervention_cost = avg_customer_value * 0.15\n",
    "success_rate = 0.35  # Higher success rate for advanced AI\n",
    "\n",
    "# Calculate business metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "final_binary_pred = (final_predictions > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, final_binary_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "customers_to_target = tp + fp\n",
    "customers_saved = tp * success_rate\n",
    "total_intervention_cost = customers_to_target * intervention_cost\n",
    "revenue_saved = customers_saved * annual_customer_value\n",
    "net_benefit = revenue_saved - total_intervention_cost\n",
    "roi = (net_benefit / total_intervention_cost) * 100 if total_intervention_cost > 0 else 0\n",
    "\n",
    "print(f\"\\nüí∞ Deep Learning Model Business Impact ({best_dl_model}):\")\n",
    "print(f\"  Model AUC: {best_dl_auc:.4f}\")\n",
    "print(f\"  Customers to Target: {customers_to_target:,}\")\n",
    "print(f\"  Expected Customers Saved: {customers_saved:.0f}\")\n",
    "print(f\"  Total Intervention Cost: ${total_intervention_cost:,.2f}\")\n",
    "print(f\"  Revenue Saved: ${revenue_saved:,.2f}\")\n",
    "print(f\"  Net Benefit: ${net_benefit:,.2f}\")\n",
    "print(f\"  ROI: {roi:.1f}%\")\n",
    "\n",
    "# Compare with baseline models\n",
    "try:\n",
    "    with open('../outputs/optimization/optimization_results.json', 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    \n",
    "    baseline_auc = baseline_results['final_auc']\n",
    "    baseline_net_benefit = baseline_results['business_impact']['net_benefit']\n",
    "    \n",
    "    auc_improvement = ((best_dl_auc - baseline_auc) / baseline_auc) * 100\n",
    "    benefit_improvement = ((net_benefit - baseline_net_benefit) / abs(baseline_net_benefit)) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Deep Learning vs Optimized Traditional Models:\")\n",
    "    print(f\"  AUC Improvement: {auc_improvement:.2f}%\")\n",
    "    print(f\"  Net Benefit Improvement: {benefit_improvement:.2f}%\")\n",
    "    \n",
    "    if auc_improvement > 0:\n",
    "        print(f\"  üéâ Deep learning models outperform traditional ML!\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Traditional ML models still competitive\")\n",
    "        \n",
    "except:\n",
    "    print(f\"\\n‚ö†Ô∏è Baseline comparison not available\")\n",
    "\n",
    "# Confidence-based segmentation\n",
    "confidence_segments = {\n",
    "    'Very High Confidence': (final_predictions >= 0.8).sum(),\n",
    "    'High Confidence': ((final_predictions >= 0.6) & (final_predictions < 0.8)).sum(),\n",
    "    'Medium Confidence': ((final_predictions >= 0.4) & (final_predictions < 0.6)).sum(),\n",
    "    'Low Confidence': (final_predictions < 0.4).sum()\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Prediction Confidence Segmentation:\")\n",
    "for segment, count in confidence_segments.items():\n",
    "    percentage = (count / len(final_predictions)) * 100\n",
    "    print(f\"  {segment}: {count:,} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model Ensemble\n",
    "print(\"üé≠ Creating Advanced Deep Learning Ensemble\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Combine predictions from all models using weighted average\n",
    "ensemble_predictions = np.zeros(len(X_test_tensor))\n",
    "total_weight = 0\n",
    "\n",
    "print(\"\\nüîÑ Computing ensemble predictions...\")\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    # Weight by AUC performance\n",
    "    weight = test_results[model_name]['AUC']\n",
    "    \n",
    "    if model_name == 'TabNet' and TABNET_AVAILABLE:\n",
    "        predictions = model.predict_proba(X_test_np)[:, 1]\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, AttentionCustomerNet):\n",
    "                predictions, _ = model(X_test_tensor)\n",
    "            else:\n",
    "                predictions = model(X_test_tensor)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    ensemble_predictions += weight * predictions\n",
    "    total_weight += weight\n",
    "    \n",
    "    print(f\"  Added {model_name} with weight {weight:.4f}\")\n",
    "\n",
    "# Normalize ensemble predictions\n",
    "ensemble_predictions /= total_weight\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_predictions)\n",
    "ensemble_binary = (ensemble_predictions > 0.5).astype(int)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_binary)\n",
    "ensemble_precision = precision_score(y_test, ensemble_binary)\n",
    "ensemble_recall = recall_score(y_test, ensemble_binary)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_binary)\n",
    "\n",
    "print(f\"\\nüéâ Deep Learning Ensemble Results:\")\n",
    "print(f\"  AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"  Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"  Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"  Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"  F1-Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "# Compare ensemble with best individual model\n",
    "ensemble_improvement = ((ensemble_auc - best_dl_auc) / best_dl_auc) * 100\n",
    "print(f\"\\nüìà Ensemble vs Best Individual Model:\")\n",
    "print(f\"  Best Individual AUC: {best_dl_auc:.4f}\")\n",
    "print(f\"  Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"  Improvement: {ensemble_improvement:.2f}%\")\n",
    "\n",
    "# Update results with ensemble\n",
    "test_results['Deep_Learning_Ensemble'] = {\n",
    "    'AUC': ensemble_auc,\n",
    "    'Accuracy': ensemble_accuracy,\n",
    "    'Precision': ensemble_precision,\n",
    "    'Recall': ensemble_recall,\n",
    "    'F1-Score': ensemble_f1,\n",
    "    'Training_Time': sum(result['Training_Time'] for result in test_results.values() if 'Training_Time' in result)\n",
    "}\n",
    "\n",
    "# Determine final best model\n",
    "final_best_auc = max(ensemble_auc, best_dl_auc)\n",
    "final_best_model = 'Deep_Learning_Ensemble' if ensemble_auc > best_dl_auc else best_dl_model\n",
    "final_predictions_best = ensemble_predictions if ensemble_auc > best_dl_auc else final_predictions\n",
    "\n",
    "print(f\"\\nüèÜ Final Best Model: {final_best_model} (AUC: {final_best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Deep Learning Models and Results\n",
    "print(\"üíæ Saving Deep Learning Models and Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import os\n",
    "os.makedirs('../models/deep_learning', exist_ok=True)\n",
    "os.makedirs('../outputs/deep_learning', exist_ok=True)\n",
    "\n",
    "# Save PyTorch models\n",
    "for model_name, model in trained_models.items():\n",
    "    if model_name != 'TabNet':\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'model_config': {\n",
    "                'input_size': input_size,\n",
    "                # Add other config parameters as needed\n",
    "            }\n",
    "        }, f'../models/deep_learning/{model_name.lower()}_model.pth')\n",
    "        print(f\"‚úÖ Saved {model_name} PyTorch model\")\n",
    "\n",
    "# Save TabNet model (if available)\n",
    "if TABNET_AVAILABLE and 'TabNet' in trained_models:\n",
    "    trained_models['TabNet'].save_model('../models/deep_learning/tabnet_model')\n",
    "    print(\"‚úÖ Saved TabNet model\")\n",
    "\n",
    "# Save autoencoder\n",
    "torch.save({\n",
    "    'model_state_dict': autoencoder.state_dict(),\n",
    "    'encoding_sizes': [64, 32, 16],\n",
    "    'input_size': input_size\n",
    "}, '../models/deep_learning/autoencoder.pth')\n",
    "print(\"‚úÖ Saved autoencoder model\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/deep_learning/deep_learning_scaler.pkl')\n",
    "print(\"‚úÖ Saved deep learning scaler\")\n",
    "\n",
    "# Save results and analysis\n",
    "deep_learning_summary = {\n",
    "    'model_performance': test_results,\n",
    "    'training_results': {k: {key: val for key, val in v.items() if key != 'train_losses' and key != 'val_losses' and key != 'val_aucs'} \n",
    "                        for k, v in training_results.items()},\n",
    "    'best_individual_model': best_dl_model,\n",
    "    'best_individual_auc': float(best_dl_auc),\n",
    "    'ensemble_auc': float(ensemble_auc),\n",
    "    'final_best_model': final_best_model,\n",
    "    'final_best_auc': float(final_best_auc),\n",
    "    'business_impact': {\n",
    "        'customers_targeted': int(customers_to_target),\n",
    "        'customers_saved': float(customers_saved),\n",
    "        'total_intervention_cost': float(total_intervention_cost),\n",
    "        'revenue_saved': float(revenue_saved),\n",
    "        'net_benefit': float(net_benefit),\n",
    "        'roi_percentage': float(roi)\n",
    "    },\n",
    "    'confidence_segments': {k: int(v) for k, v in confidence_segments.items()},\n",
    "    'feature_importance': importance_df.to_dict('records') if 'importance_df' in locals() else [],\n",
    "    'autoencoder_performance': {\n",
    "        'final_train_loss': ae_train_losses[-1] if ae_train_losses else 0,\n",
    "        'final_val_loss': ae_val_losses[-1] if ae_val_losses else 0,\n",
    "        'compression_ratio': f\"{input_size} -> {X_train_encoded.shape[1]}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deep learning summary\n",
    "with open('../outputs/deep_learning/deep_learning_results.json', 'w') as f:\n",
    "    json.dump(deep_learning_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Saved deep learning results\")\n",
    "\n",
    "# Save model comparison\n",
    "final_results_df = pd.DataFrame(test_results).T\n",
    "final_results_df.to_csv('../outputs/deep_learning/model_comparison.csv')\n",
    "print(\"‚úÖ Saved model comparison\")\n",
    "\n",
    "# Save predictions for ensemble\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test.values,\n",
    "    'ensemble_predictions': ensemble_predictions,\n",
    "    'best_individual_predictions': final_predictions,\n",
    "    'confidence_level': pd.cut(ensemble_predictions, \n",
    "                              bins=[0, 0.4, 0.6, 0.8, 1.0], \n",
    "                              labels=['Low', 'Medium', 'High', 'Very High'], include_lowest=True)\n",
    "})\n",
    "predictions_df.to_csv('../outputs/deep_learning/predictions.csv', index=False)\n",
    "print(\"‚úÖ Saved predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comprehensive Deep Learning Report\n",
    "print(\"üìã Creating Comprehensive Deep Learning Report\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Calculate additional insights\n",
    "total_training_time = sum(result['Training_Time'] for result in test_results.values() if 'Training_Time' in result)\n",
    "avg_auc = np.mean([result['AUC'] for result in test_results.values()])\n",
    "model_count = len(test_results) - 1  # Exclude ensemble\n",
    "\n",
    "# GPU utilization info\n",
    "gpu_info = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_used = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    gpu_info = f\"\\nüî• GPU UTILIZATION:\\n‚Ä¢ Max Memory Used: {gpu_memory_used:.2f} GB\\n‚Ä¢ Device: {torch.cuda.get_device_name(0)}\"\n",
    "\n",
    "# Create comprehensive report\n",
    "deep_learning_report = f\"\"\"\n",
    "üéâ DEEP LEARNING MODELS TRAINING COMPLETED!\n",
    "======================================================================\n",
    "\n",
    "üß† MODELS TRAINED:\n",
    "‚Ä¢ Deep Neural Networks: {model_count}\n",
    "‚Ä¢ Autoencoder: 1 (Feature learning)\n",
    "‚Ä¢ TabNet: {'1' if TABNET_AVAILABLE else '0 (not available)'}\n",
    "‚Ä¢ Ensemble Model: 1 (Weighted combination)\n",
    "‚Ä¢ Total Training Time: {total_training_time:.1f} seconds\n",
    "\n",
    "üèÜ PERFORMANCE RESULTS:\n",
    "‚Ä¢ Best Individual Model: {best_dl_model} (AUC: {best_dl_auc:.4f})\n",
    "‚Ä¢ Ensemble Model AUC: {ensemble_auc:.4f}\n",
    "‚Ä¢ Final Best Model: {final_best_model} (AUC: {final_best_auc:.4f})\n",
    "‚Ä¢ Average Model AUC: {avg_auc:.4f}\n",
    "‚Ä¢ Ensemble Improvement: {ensemble_improvement:.2f}%\n",
    "\n",
    "üéØ MODEL ARCHITECTURES:\n",
    "‚Ä¢ DeepCustomerNet: Multi-layer perceptron with batch normalization\n",
    "‚Ä¢ AttentionNet: Multi-head attention mechanism for feature importance\n",
    "‚Ä¢ ResidualNet: Residual connections for deep learning\n",
    "‚Ä¢ AutoencoderNet: Compressed feature representation learning\n",
    "{'‚Ä¢ TabNet: Attention-based tabular deep learning' if TABNET_AVAILABLE else ''}\n",
    "\n",
    "üîÑ AUTOENCODER INSIGHTS:\n",
    "‚Ä¢ Feature Compression: {input_size} -> {X_train_encoded.shape[1]} dimensions\n",
    "‚Ä¢ Final Training Loss: {ae_train_losses[-1]:.4f}\n",
    "‚Ä¢ Final Validation Loss: {ae_val_losses[-1]:.4f}\n",
    "‚Ä¢ Compression Ratio: {(1 - X_train_encoded.shape[1]/input_size)*100:.1f}% reduction\n",
    "\n",
    "üíº BUSINESS IMPACT (BEST MODEL):\n",
    "‚Ä¢ Customers to Target: {customers_to_target:,}\n",
    "‚Ä¢ Expected Customers Saved: {customers_saved:.0f}\n",
    "‚Ä¢ Total Intervention Cost: ${total_intervention_cost:,.2f}\n",
    "‚Ä¢ Revenue Saved: ${revenue_saved:,.2f}\n",
    "‚Ä¢ Net Benefit: ${net_benefit:,.2f}\n",
    "‚Ä¢ ROI: {roi:.1f}%\n",
    "\n",
    "üéØ PREDICTION CONFIDENCE:\n",
    "‚Ä¢ Very High Confidence: {confidence_segments['Very High Confidence']:,} customers\n",
    "‚Ä¢ High Confidence: {confidence_segments['High Confidence']:,} customers\n",
    "‚Ä¢ Medium Confidence: {confidence_segments['Medium Confidence']:,} customers\n",
    "‚Ä¢ Low Confidence: {confidence_segments['Low Confidence']:,} customers\n",
    "{gpu_info}\n",
    "\n",
    "üìÅ SAVED ARTIFACTS:\n",
    "‚Ä¢ PyTorch Models: ../models/deep_learning/\n",
    "‚Ä¢ Autoencoder: ../models/deep_learning/autoencoder.pth\n",
    "{'‚Ä¢ TabNet Model: ../models/deep_learning/tabnet_model/' if TABNET_AVAILABLE else ''}\n",
    "‚Ä¢ Scaler: ../models/deep_learning/deep_learning_scaler.pkl\n",
    "‚Ä¢ Results: ../outputs/deep_learning/deep_learning_results.json\n",
    "‚Ä¢ Predictions: ../outputs/deep_learning/predictions.csv\n",
    "‚Ä¢ Model Comparison: ../outputs/deep_learning/model_comparison.csv\n",
    "\n",
    "üîç KEY INSIGHTS:\n",
    "‚Ä¢ {'Deep learning outperforms traditional ML' if 'auc_improvement' in locals() and auc_improvement > 0 else 'Traditional ML remains competitive'}\n",
    "‚Ä¢ Attention mechanisms {'provide' if 'AttentionNet' in test_results else 'could provide'} interpretable feature importance\n",
    "‚Ä¢ Autoencoder successfully compressed features by {(1 - X_train_encoded.shape[1]/input_size)*100:.1f}%\n",
    "‚Ä¢ Ensemble approach {'improved' if ensemble_improvement > 0 else 'maintained'} performance\n",
    "‚Ä¢ GPU acceleration {'enabled' if torch.cuda.is_available() else 'not available'} for faster training\n",
    "\n",
    "üöÄ NEXT STEPS:\n",
    "1. Deploy best performing model to production\n",
    "2. Implement real-time inference pipeline\n",
    "3. Set up model monitoring and drift detection\n",
    "4. Create A/B testing framework for model comparison\n",
    "5. Implement automated retraining with new data\n",
    "6. Develop model interpretability dashboard\n",
    "7. Scale inference for high-throughput predictions\n",
    "\n",
    "üí° ADVANCED FEATURES IMPLEMENTED:\n",
    "‚úÖ Custom neural network architectures\n",
    "‚úÖ Attention mechanisms for interpretability\n",
    "‚úÖ Residual connections for deep networks\n",
    "‚úÖ Autoencoder for feature learning\n",
    "‚úÖ Advanced regularization (dropout, batch norm)\n",
    "‚úÖ Focal loss for class imbalance\n",
    "‚úÖ Learning rate scheduling\n",
    "‚úÖ Early stopping\n",
    "‚úÖ Gradient clipping\n",
    "‚úÖ Model ensembling\n",
    "{'‚úÖ TabNet implementation' if TABNET_AVAILABLE else '‚ùå TabNet not available'}\n",
    "‚úÖ GPU acceleration\n",
    "‚úÖ Feature importance analysis\n",
    "‚úÖ Business impact calculation\n",
    "\"\"\"\n",
    "\n",
    "print(deep_learning_report)\n",
    "\n",
    "# Save report\n",
    "with open('../outputs/deep_learning/deep_learning_summary.txt', 'w') as f:\n",
    "    f.write(deep_learning_report)\n",
    "\n",
    "print(\"‚úÖ Deep learning summary saved to ../outputs/deep_learning/deep_learning_summary.txt\")\n",
    "print(\"\\nüéâ Deep learning model development completed successfully!\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
